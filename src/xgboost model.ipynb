{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Bank's Term Deposit Subscription - XGBoost Classification Model\n",
    "\n",
    "#### Author: Guansu(Frances) Niu\n",
    "\n",
    "#### Data Resource: https://archive.ics.uci.edu/ml/datasets/Bank+Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data:\n",
    "\n",
    "raw_data = pd.read_csv(\"data/raw data.csv\",sep=';')\n",
    "preprocessed_data = pd.read_csv(\"data/preprocessed data.csv\")\n",
    "\n",
    "pp_data = preprocessed_data.drop(preprocessed_data.columns[0], axis=1)\n",
    "\n",
    "X = pp_data.drop(['y'], inplace = False, axis = 1)\n",
    "y = pp_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.202432155099011\n"
     ]
    }
   ],
   "source": [
    "# Calculate baseline of F1 score:\n",
    "\n",
    "# Method reference: \n",
    "# https://stats.stackexchange.com/questions/390200/what-is-the-baseline-of-the-f1-score-for-a-binary-classifier\n",
    "\n",
    "pp_data['y'].value_counts()\n",
    "count_0 = 36531\n",
    "prob_0 = count_0/len(pp_data['y'])\n",
    "prob_1 = (len(pp_data['y']) - count_0)/len(pp_data['y'])\n",
    "baseline = (2*prob_1)/(prob_1+1)\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data:\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=random_state,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing:\n",
    "\n",
    "cat_ftrs = ['job', 'marital', 'default', 'housing','loan', 'contact','poutcome']\n",
    "\n",
    "ordinal_ftrs = ['education','month','day_of_week']\n",
    "\n",
    "ordinal_cats = [['basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate', 'professional.course', \n",
    "                'university.degree','missing'],['mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep','oct',\n",
    "                'nov','dec'],['mon', 'tue', 'wed', 'thu', 'fri']]\n",
    "\n",
    "num_ftrs = ['age','campaign','previous','emp.var.rate','cons.price.idx','cons.conf.idx', \n",
    "                 'euribor3m','nr.employed']\n",
    "\n",
    "# one-hot encoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant',fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'))])\n",
    "\n",
    "# ordinal encoder\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer2', SimpleImputer(strategy='constant',fill_value='NA')),\n",
    "    ('ordinal', OrdinalEncoder(categories = ordinal_cats))])\n",
    "\n",
    "# standard scaler\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# collect the transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_ftrs),\n",
    "        ('cat', categorical_transformer, cat_ftrs),\n",
    "        ('ord', ordinal_transformer, ordinal_ftrs)])\n",
    "\n",
    "# fit_transform the training set\n",
    "X_prep = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Collect feature names\n",
    "\n",
    "feature_names = preprocessor.transformers_[0][-1] + \\\n",
    "                list(preprocessor.named_transformers_['cat'][1].get_feature_names(cat_ftrs)) + \\\n",
    "                preprocessor.transformers_[2][-1]\n",
    "\n",
    "X_train = pd.DataFrame(data=X_prep,columns=feature_names)\n",
    "\n",
    "# transform the test\n",
    "df_test = preprocessor.transform(X_test)\n",
    "X_test = pd.DataFrame(data=df_test,columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost: max_depth=11,f1_score=0.3927536231884058\n"
     ]
    }
   ],
   "source": [
    "# Tune parameters and get best F1 score:\n",
    "\n",
    "max_depths = range(7,14) \n",
    "f1_xgb=[]\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    xgb = XGBClassifier(max_depth=max_depth,learning_rate=0.1,random_state=random_state)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    y_pred_xgb = xgb.predict(X_test)\n",
    "    f1_xgb.append(fbeta_score(y_test, y_pred_xgb,1))\n",
    "    \n",
    "best_max_depth = max_depths[np.argmax(f1_xgb)]\n",
    "\n",
    "print(f\"xgboost: max_depth={best_max_depth},f1_score={max(f1_xgb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37904190273634375\n"
     ]
    }
   ],
   "source": [
    "# Cross validation using parameters tuned from the best f1 score:\n",
    "\n",
    "xgb = XGBClassifier(max_depth=11, learning_rate=0.1,random_state=random_state)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=10,shuffle=True,random_state=1)\n",
    "CV = (cross_val_score(xgb, X_train, y_train, cv=kf, n_jobs=1, scoring = 'f1').mean())\n",
    "print(CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007599212627488509\n"
     ]
    }
   ],
   "source": [
    "# Uncertainty due to splitting:\n",
    "\n",
    "uncertainty_split_xgb = []\n",
    "\n",
    "for i in range(1, 12, 2): \n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=i,stratify=y)\n",
    "\n",
    "    # Run the model using the tuned parameters:\n",
    "    xgb = XGBClassifier(max_depth=11,learning_rate=0.001,random_state=123)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "    # Cross validation\n",
    "    CV = (cross_val_score(xgb, X_train, y_train, cv=kf, n_jobs=1, scoring = 'f1').mean())\n",
    "    uncertainty_split_xgb.append(CV)\n",
    "print(np.std(uncertainty_split_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Uncertainty due to non-deterministic ML methods:\n",
    "\n",
    "uncertainty_ndeter_xgb = []\n",
    "\n",
    "for i in range(123, 500, 80): \n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=58,stratify=y)\n",
    "\n",
    "    # Run the model using the tuned parameters:\n",
    "    xgb = XGBClassifier(max_depth=11,learning_rate=0.001,random_state=i)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "    # Cross validation\n",
    "    CV = (cross_val_score(xgb, X_train, y_train, cv=kf, n_jobs=1, scoring = 'f1').mean())\n",
    "    uncertainty_ndeter_xgb.append(CV)\n",
    "print(np.std(uncertainty_ndeter_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7167,  140],\n",
       "       [ 690,  237]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix:\n",
    "\n",
    "confusion_matrix(y_test,y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8982268642215205"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score:\n",
    "\n",
    "accuracy_score(y_test, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8873855272426944\n"
     ]
    }
   ],
   "source": [
    "# Baseline of accuracy score:\n",
    "\n",
    "baseline_accuracy = np.array(pp_data[\"y\"].value_counts()/pp_data.shape[0])[0]\n",
    "print(baseline_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For xgboost classification:\n",
    "\n",
    "The F1 score is 0.379 (after cross validation) with tuned parameters max_depth = 11.\n",
    "\n",
    "The uncertainties due to splitting is 0.00760, and due to non-deterministic ML method is 0.0.\n",
    "\n",
    "The accuracy score is 0.898."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
